# Summarization + Tool Retry Implementation Plan

**Date:** 2026-01-20  
**Goal:** Add context window protection + automatic tool retry to prevent crashes and improve reliability  
**Approach:** LangGraph middleware (simple, production-ready)

---

## ğŸ“‹ TODO List Summary

### Phase 1: Setup & Dependencies (Tasks 1-2)
- âœ… Analysis complete (see `CHECKPOINT_ARCHITECTURE_ANALYSIS.md`)
- â³ Install/verify langchain.agents package
- â³ Backup main_graph.py

### Phase 2: Add Middleware to Graph (Tasks 3-9)
- â³ Import SummarizationMiddleware
- â³ Import ToolRetryMiddleware
- â³ Update build_main_graph() function
- â³ Configure summarization (8k tokens, keep 6 messages)
- â³ Configure tool retry (2 attempts, 1.5s backoff)
- â³ Add logging for visibility

### Phase 3: Testing (Tasks 10-14)
- â³ Short conversation test (no summarization)
- â³ Long conversation test (triggers summarization)
- â³ Verify checkpoint persistence
- â³ Test tool retry behavior
- â³ Verify retry backoff timing

### Phase 4: Documentation & UI (Tasks 15-16)
- â³ Document implementation
- â³ Update views.py to expose middleware events

---

## ğŸ¯ Implementation Approach

### **Option A: StateGraph + Custom Middleware Wrapper** (RECOMMENDED)

**Why:** Keeps our existing graph structure (fast paths, routing) while adding middleware

```python
# backend/llm/graphs/main_graph.py

from langgraph.graph import StateGraph, START, END
from langchain.agents.middleware import SummarizationMiddleware, ToolRetryMiddleware

# Build graph as before
builder = StateGraph(MainWorkflowState)
builder.add_node("agent", agent_node)
builder.add_node("tools", tools_node)
# ... add other nodes (fast paths, etc.)

# NEW: Wrap compilation with middleware
if use_checkpointer:
    checkpointer = await create_checkpointer_for_current_loop()
    
    # Create middleware list
    middleware = [
        SummarizationMiddleware(
            model="gpt-4o-mini",  # Cheap model for summaries
            trigger={"tokens": 8000},  # Summarize when >8k tokens
            keep={"messages": 6},  # Keep last 6 messages raw
            system_prompt="""Summarize this conversation concisely:
            - User's goals and questions
            - Key facts discovered (property details, valuations)
            - Open questions
            Keep under 300 words."""
        ),
        ToolRetryMiddleware(
            max_retries=2,  # Retry failed tools twice
            backoff=1.5,  # 1s, 1.5s, 2.25s delays
            retry_on=["timeout", "connection_error", "rate_limit"]
        )
    ]
    
    # Compile with checkpointer + middleware
    main_graph = builder.compile(
        checkpointer=checkpointer,
        # NOTE: StateGraph.compile() may not directly accept middleware
        # If not, we'll use a wrapper approach (see below)
    )
```

**âš ï¸ Important Check:** `StateGraph.compile()` signature

If `StateGraph.compile()` doesn't accept `middleware` directly, we'll use **wrapper approach**:

```python
# Wrapper approach (if needed)
from langgraph.graph import StateGraph

# Build graph
base_graph = builder.compile(checkpointer=checkpointer)

# Wrap with middleware
from langchain.agents.middleware import apply_middleware

main_graph = apply_middleware(
    base_graph,
    middleware=[summarization_middleware, tool_retry_middleware]
)
```

---

## ğŸ“ Step-by-Step Implementation

### Step 1: Check Dependencies

**File:** `requirements.txt`

**Check for:**
```txt
langchain>=0.3.0
langchain-openai>=0.2.0
langgraph>=0.2.0
```

**If missing, add:**
```bash
pip install langchain langchain-openai langgraph --upgrade
```

---

### Step 2: Backup Current Graph

**Command:**
```bash
cp backend/llm/graphs/main_graph.py backend/llm/graphs/main_graph.py.backup
```

---

### Step 3: Update main_graph.py Imports

**File:** `backend/llm/graphs/main_graph.py`

**Add after line 38:**
```python
# NEW: Middleware for context management and retry logic
try:
    from langchain.agents.middleware import (
        SummarizationMiddleware,
        ToolRetryMiddleware
    )
    MIDDLEWARE_AVAILABLE = True
    logger.info("âœ… LangChain middleware available")
except ImportError as e:
    MIDDLEWARE_AVAILABLE = False
    SummarizationMiddleware = None
    ToolRetryMiddleware = None
    logger.warning(f"âš ï¸ LangChain middleware not available: {e}")
```

---

### Step 4: Create Middleware Configuration Function

**File:** `backend/llm/graphs/main_graph.py`

**Add before `build_main_graph()` function:**
```python
def create_middleware_config() -> list:
    """
    Create middleware configuration for the agent.
    Returns list of middleware instances or empty list if unavailable.
    """
    if not MIDDLEWARE_AVAILABLE:
        logger.warning("Middleware not available - returning empty list")
        return []
    
    middleware = []
    
    # 1. Summarization Middleware (prevent token overflow)
    try:
        summarization = SummarizationMiddleware(
            model="gpt-4o-mini",  # Cheap model for summaries
            trigger={"tokens": 8000},  # Trigger at 8k tokens (~50-60 messages)
            keep={"messages": 6},  # Keep last 6 messages raw for context
            system_prompt="""Summarize the conversation history concisely.

Focus on:
1. User's primary questions and goals
2. Key facts discovered (property addresses, valuations, dates)
3. Documents referenced and their relevance
4. Open questions or unresolved issues

Keep the summary under 300 words. Be specific and factual."""
        )
        middleware.append(summarization)
        logger.info("âœ… Added SummarizationMiddleware (trigger: 8k tokens, keep: 6 msgs)")
    except Exception as e:
        logger.error(f"âŒ Failed to create SummarizationMiddleware: {e}")
    
    # 2. Tool Retry Middleware (auto-retry failures)
    try:
        tool_retry = ToolRetryMiddleware(
            max_retries=2,  # Retry up to 2 times (3 total attempts)
            backoff=1.5,  # Exponential backoff multiplier
            retry_on=[
                "timeout",
                "connection_error",
                "rate_limit",
                "502",  # Bad Gateway
                "503",  # Service Unavailable
                "504"   # Gateway Timeout
            ]
        )
        middleware.append(tool_retry)
        logger.info("âœ… Added ToolRetryMiddleware (max_retries: 2, backoff: 1.5x)")
    except Exception as e:
        logger.error(f"âŒ Failed to create ToolRetryMiddleware: {e}")
    
    return middleware
```

---

### Step 5: Update build_main_graph() Function

**File:** `backend/llm/graphs/main_graph.py`

**Find the compilation section (around line 510) and update:**

**BEFORE:**
```python
if use_checkpointer:
    checkpointer = await create_checkpointer_for_current_loop()
    main_graph = builder.compile(checkpointer=checkpointer)
    logger.info("Graph compiled with checkpointer")
    return main_graph, checkpointer
else:
    main_graph = builder.compile()
    return main_graph, checkpointer
```

**AFTER:**
```python
if use_checkpointer:
    checkpointer = await create_checkpointer_for_current_loop()
    
    # NEW: Create middleware config
    middleware = create_middleware_config()
    
    if middleware:
        logger.info(f"ğŸ“¦ Compiling graph with checkpointer + {len(middleware)} middleware")
        
        # Check if StateGraph.compile() accepts middleware parameter
        import inspect
        compile_sig = inspect.signature(builder.compile)
        
        if 'middleware' in compile_sig.parameters:
            # Direct middleware support
            main_graph = builder.compile(
                checkpointer=checkpointer,
                middleware=middleware
            )
            logger.info("âœ… Graph compiled with checkpointer + middleware (direct)")
        else:
            # Fallback: compile without middleware, apply wrapper if available
            logger.warning("âš ï¸ StateGraph.compile() doesn't accept middleware parameter")
            logger.warning("   Compiling without middleware - manual implementation needed")
            main_graph = builder.compile(checkpointer=checkpointer)
    else:
        logger.info("ğŸ“¦ Compiling graph with checkpointer only (no middleware)")
        main_graph = builder.compile(checkpointer=checkpointer)
    
    logger.info("Graph compiled successfully")
    return main_graph, checkpointer
else:
    main_graph = builder.compile()
    return main_graph, checkpointer
```

---

### Step 6: Add Logging Hooks for Middleware

**File:** `backend/llm/nodes/agent_node.py`

**Add after message deduplication logic (around line 56):**
```python
# Log message count for summarization tracking
logger.info(f"[AGENT_NODE] Message history: {len(messages)} messages")

# Estimate token count (rough approximation)
estimated_tokens = sum(len(str(msg.content)) // 4 for msg in messages)
logger.info(f"[AGENT_NODE] Estimated tokens: {estimated_tokens}")

if estimated_tokens > 8000:
    logger.warning(f"âš ï¸ [AGENT_NODE] Token count ({estimated_tokens}) exceeds 8k - summarization should trigger!")
```

---

### Step 7: Update views.py to Expose Middleware Events

**File:** `backend/views.py`

**In the event streaming loop (around line 1326), add:**
```python
# Inside the async for event in graph.astream_events(...) loop
event_type = event.get("event")

# NEW: Capture middleware events
if event_type == "on_middleware_start":
    middleware_name = event.get("name", "unknown")
    logger.info(f"ğŸ”§ [MIDDLEWARE] {middleware_name} started")
    
    # Emit to frontend
    yield f"data: {json.dumps({'type': 'middleware', 'name': middleware_name, 'status': 'started'})}\n\n"

elif event_type == "on_middleware_end":
    middleware_name = event.get("name", "unknown")
    middleware_output = event.get("data", {}).get("output", {})
    logger.info(f"âœ… [MIDDLEWARE] {middleware_name} completed")
    
    # If summarization occurred, notify user
    if "summarization" in middleware_name.lower():
        summary_length = len(str(middleware_output.get("summary", "")))
        logger.info(f"ğŸ“ [MIDDLEWARE] Context summarized ({summary_length} chars)")
        
        yield f"data: {json.dumps({'type': 'context_summarized', 'summary_length': summary_length})}\n\n"
```

---

## ğŸ§ª Testing Plan

### Test 1: Short Conversation (No Summarization)

**Goal:** Verify middleware doesn't trigger for short conversations

**Steps:**
1. Start new chat
2. Send 5-10 questions
3. Check logs for: `Message history: N messages` (should be <20)
4. Check logs for: `Estimated tokens: N` (should be <8000)
5. **Expected:** No summarization trigger

**Pass Criteria:**
- âœ… No "summarization should trigger" warning
- âœ… All messages preserved in response
- âœ… No middleware events in logs

---

### Test 2: Long Conversation (Triggers Summarization)

**Goal:** Verify summarization activates at 8k tokens

**Steps:**
1. Start new chat
2. Send 30-50 questions (detailed, long responses)
3. Monitor logs for token count climbing
4. **Expected:** After ~40-50 messages, see summarization trigger

**Pass Criteria:**
- âœ… See "summarization should trigger" warning
- âœ… See middleware event: `[MIDDLEWARE] SummarizationMiddleware started`
- âœ… See `Context summarized` log
- âœ… Next response still coherent (summary preserved context)

**Verification Query:**
```sql
-- Check checkpoint size after summarization
SELECT 
    thread_id,
    checkpoint_id,
    created_at,
    LENGTH(checkpoint::text) as size_bytes
FROM checkpoints
WHERE thread_id = 'YOUR_THREAD_ID'
ORDER BY created_at DESC
LIMIT 10;
```

**Expected:** Checkpoint size should **drop** after summarization (fewer messages)

---

### Test 3: Tool Retry on Failure

**Goal:** Verify tools automatically retry on failure

**Steps:**
1. Simulate DB timeout (or disconnect Supabase temporarily)
2. Ask question that triggers `retrieve_documents` tool
3. Check logs for retry attempts

**Pass Criteria:**
- âœ… See `[TOOL_RETRY] Attempt 1 failed, retrying...`
- âœ… See `[TOOL_RETRY] Attempt 2 failed, retrying...`
- âœ… See backoff delays (1s, 1.5s, 2.25s)
- âœ… After 3 attempts, agent receives error message

---

### Test 4: Verify Checkpoint Persistence

**Goal:** Summaries persist across sessions

**Steps:**
1. Create long conversation (trigger summarization)
2. Close chat
3. Resume chat (same `sessionId`)
4. Check that context is preserved

**Pass Criteria:**
- âœ… Agent remembers earlier conversation
- âœ… Checkpoints contain summarized messages
- âœ… No duplicate summaries on resume

---

## ğŸ“Š Expected Behavior

### Before Middleware

**Conversation:**
```
Turn 1: User asks about property â†’ 2 messages
Turn 2: Agent retrieves docs â†’ 4 messages
...
Turn 100: User asks follow-up â†’ 200 messages
Turn 200: ğŸ’¥ CRASH - Token limit exceeded!
```

**Token Growth:**
- Linear growth: ~200 tokens per turn
- No summarization
- Crashes after ~300 turns

---

### After Middleware

**Conversation:**
```
Turn 1-50: Normal (2-100 messages, <8k tokens)
Turn 51: Summarization triggered! (8k tokens reached)
  â†’ Old messages (1-45) â†’ Summary (1 message, 500 tokens)
  â†’ Recent messages (46-51) â†’ Kept raw (6 messages)
  â†’ New message count: 7 messages (summary + 6 recent)
Turn 52-100: Continue with summarized context
Turn 101: Another summarization (if needed)
...
Turn 1000: Still working! ğŸ‰
```

**Token Growth:**
- Capped at ~8k tokens
- Periodic summarization every ~50 turns
- **Unlimited conversation length!**

---

## ğŸ”§ Troubleshooting

### Issue: Middleware Not Available

**Error:** `ImportError: cannot import name 'SummarizationMiddleware'`

**Fix:**
```bash
pip install --upgrade langchain langchain-openai
# Check version: should be >=0.3.0
python -c "import langchain; print(langchain.__version__)"
```

---

### Issue: StateGraph.compile() Doesn't Accept Middleware

**Error:** `compile() got an unexpected keyword argument 'middleware'`

**Fix:** Use custom node approach instead:

```python
# backend/llm/nodes/context_manager_node.py

async def context_manager_node(state: MainWorkflowState) -> MainWorkflowState:
    """Manual summarization node (if middleware unavailable)"""
    messages = state.get("messages", [])
    
    # Count tokens
    tokens = sum(len(str(msg.content)) // 4 for msg in messages)
    
    if tokens < 8000:
        return {}  # No change needed
    
    # Summarize old messages
    old_messages = messages[:-6]
    recent_messages = messages[-6:]
    
    # Call LLM to summarize
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    summary_prompt = f"Summarize this conversation: {old_messages}"
    summary = await llm.ainvoke([HumanMessage(content=summary_prompt)])
    
    return {
        "messages": [SystemMessage(content=f"[SUMMARY] {summary.content}")] + recent_messages
    }
```

**Add to graph:**
```python
builder.add_node("context_manager", context_manager_node)
builder.add_conditional_edges("context_manager", lambda s: "agent")
```

---

## ğŸ“ˆ Success Metrics

### Quantitative

| Metric | Before | After | Target |
|--------|--------|-------|--------|
| Max conversation length | 200 turns | 1000+ turns | âœ… Unlimited |
| Avg tokens per turn | 200-500 | 200-500 (capped) | âœ… <8k total |
| Tool failure rate | 15% | 5% | âœ… <10% |
| Auto-recovery rate | 0% | 66% | âœ… >50% |

### Qualitative

- âœ… No more "token limit exceeded" errors
- âœ… Long conversations work seamlessly
- âœ… Context preserved across summarizations
- âœ… Tools retry automatically on transient failures
- âœ… User doesn't notice summarization happening

---

## ğŸ¯ Next Steps After Implementation

1. **Phase 2B: Session CRUD APIs**
   - Populate `chat_sessions` table
   - Create `/api/llm/sessions` endpoints
   - Sync frontend with backend

2. **Phase 3: Long-Term Memory Store**
   - Add `PostgresStore` for facts
   - Create memory tools (remember/recall)
   - Persist user preferences

3. **Phase 4: Context Summarization UI**
   - Show badge when context summarized
   - Allow users to view summary
   - Add "Start fresh" button to reset context

---

## ğŸ“š References

- [LangGraph Middleware Docs](https://langchain-ai.github.io/langgraph/how-tos/middleware/)
- [Summarization Middleware API](https://api.python.langchain.com/en/latest/agents/langchain.agents.middleware.SummarizationMiddleware.html)
- [Tool Retry Middleware API](https://api.python.langchain.com/en/latest/agents/langchain.agents.middleware.ToolRetryMiddleware.html)
- [LangGraph Checkpointer](https://langchain-ai.github.io/langgraph/how-tos/persistence/)

---

**Ready to implement! Next: Check dependencies and start coding** ğŸš€

